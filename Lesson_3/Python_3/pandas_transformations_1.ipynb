{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêç.3 Pandas Data Transformations, Pt. 1\n",
    "_Nate Robinson_\n",
    "\n",
    "Today's lesson will focus on understanding how we can use pandas to cut down on data manipulation workload and introduce some common methods for transforming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We should append the custom module to our PYTHONPATH to have access\n",
    "# to the db_utils module!\n",
    "sys.path.append('../../custom')\n",
    "\n",
    "from db_utils import get_connection, validate_connection, get_data\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating Question\n",
    "\n",
    "How can we manipulate data in `pandas` to to answer our questions?\n",
    "What are some common data manipulations steps we should feel comfortable with in pandas?\n",
    "There's more than one way to skin a cat! üôÄ\n",
    "\n",
    "We're going to be working with some **Milestone Updates** data which is somewhat messy but the only source for a lot of questions we'll need to solve.\n",
    "\n",
    "*Note: the column `update_lead_hours` here is a calculation of hours between when the update was made, and when the milestone occured. Negative means the milestone was created after it happened, postive means the milestone was created ahead of the actual event.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll get started by establishing a database connection and pulling a predetermined dataset of milestone updates\n",
    "# We're selecting for only July 2020 and only arrival/departure milestones to ignore T&T events like last free day.\n",
    "conn, cur = get_connection()\n",
    "df = get_data('milestones.sql','file', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "\n",
    "We've got four questions ahead of us. The first two we'll do here:\n",
    "\n",
    "    1. What percentage of actual updates are human vs automated\n",
    "    2. What (if any) is the improvement in update speed earned by automation and operations teams\n",
    "    \n",
    "The latter two will be explored in Part 2 of this lesson:\n",
    "    \n",
    "    3. What milestones are frequently missing per given mode?\n",
    "    4. What are the fastest updates per mode\n",
    "    \n",
    "Keeping in mind for the above, we'll need to handle outliers and extraneous/missing data.\n",
    "   \n",
    "## Data Exploration\n",
    "\n",
    "First, let's take a look through our data to have an idea of what we're working with.\n",
    "\n",
    "**Can we identify any issues we might have in attempting to answer our motivating questions?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question? Explore the data and determine some issues we may see with this analysis\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Percentage of Updates: Human vs. Automated\n",
    "\n",
    "I'm solving this by stringing together some logic to **only solve for air and ocean milestones** (that's all we want right now) and **choosing only actual dates, not scheduled**. \n",
    "\n",
    "From there we get a total count of rows, and final determine the count of each value in a series before dividing by the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One, less-readable way to filter data is to create a series that holds conditional expressions\n",
    "# for example\n",
    "ocean_filter = df.shipment_mode == 'Ocean'\n",
    "air_filter = df.shipment_mode == 'Air'\n",
    "milestones_filter = df.update_date_type == 'actual'\n",
    "final_filter =  (ocean_filter | air_filter ) & milestones_filter\n",
    "\n",
    "# We'll stop to discuss what these above filters mean here.\n",
    "total_rows = df[final_filter]['id'].count()\n",
    "df[final_filter]['source'].value_counts()/total_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Work Session\n",
    "\n",
    "After examining this solution work through your own solution utilizing another way to cut data. This could be solved using `iloc`, `loc`, or <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html\">query which I suggest</a>. You can also use the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html\">groupby function</a> to circumvent `value_counts`.\n",
    "\n",
    "Try to answer this question in the most Pythonic, efficient, or visually appealing way; for example, try answering this question with one line of code! Furthermore, think about how we could persist this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Improvement in Update Speed for Automation\n",
    "\n",
    "Well, we learned by answering the prior question that the majority of our updates are coming from humans, with a bit of our updates provided by `crux` or `inttra` or `cargosmart`. Nearly 30% fell into the `no user found` category. If you dig a little bit deeper, you'll find that we also have some null values present in this column. \n",
    "\n",
    "In this next challenge, we're going to do a handful of things:\n",
    "    \n",
    "    1. Use a separate CSV which maps \"Human\" users to bots, individuals or groups\n",
    "    2. Using logic source and user type, determine if the the update was a bot, individual, or shared account\n",
    "    3. Drop null values\n",
    "    4. Eliminate outlier data and calculate average update speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve user/type mapping\n",
    "user_types = pd.read_csv('user_types.csv')\n",
    "\n",
    "# Create a new column with user_type in our current DF\n",
    "data = df[final_filter]\n",
    "data = pd.merge(data , user_types, on='created_by', how='left')\n",
    "\n",
    "# Drop rows where the created_by or source are null\n",
    "data = data.dropna(how='any', subset=['source','created_by'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for determining what type of user we have\n",
    "def get_user(row):\n",
    "    if row['source'] in ('crux', 'inttra', 'cargosmart'):\n",
    "        return 'bot'\n",
    "    if (row['source'] == 'no user found'):\n",
    "        return row['user_type']\n",
    "    if (row['source'] == 'human'):\n",
    "        return row['user_type']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply this function to our dataframe to create a new column from our logic\n",
    "data['categorical_user_type'] = data.apply(get_user, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll then want to drop all null values from this colume that we'll be using moving forward\n",
    "data = data.dropna(how='any', subset=['categorical_user_type'])\n",
    "\n",
    "# Now let's take a look at what we're working with\n",
    "data['update_lead_hours'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers\n",
    "\n",
    "We see that the long-tail of update lead hours is very long. On average, our milestone updates are logged $313$ hours after occuring, with the median sitting at $20$ hours. For this analysis we can probably assume no \"actual\" updates should be happening before the fact, so we'll drop all values greater than $0$. \n",
    "\n",
    "It seems like some of our large outliers are significantly skewing our data. I think 2 weeks is a reasonable assumption to make for the most amount of time it should take to update a milestone -- we'll drop all rows with lead hours over $336$ hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = data[(data['update_lead_hours'] < 0) & (data['update_lead_hours'] > -336.0)]\n",
    "\n",
    "# our final dataset has significantly less values\n",
    "final_dataset['id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.groupby(by='categorical_user_type')['update_lead_hours'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "This gives us the expected solution! Bots are significantly faster than human users, and accounts run by individuals are slower at making updates than those owned by shared accounts (teams).\n",
    "\n",
    "### Group Work\n",
    "\n",
    "We've walked through a single method for arriving at this solution. \n",
    "**Can you arrive at this solution through another method? Is there any other relevant measures we could calculate to provide more insight?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
