{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem\n",
    "DA Probability & Statistics â€¢ Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics to Cover\n",
    "- Motivating question - Together\n",
    "- Bayes intuition for the 2 event case - NJ\n",
    "- Updating priors - Shreyas\n",
    "    - Loop through various priors to demonstrate how the posterior probabilities will update\n",
    "    - What is the consequence of updating priors?\n",
    "    - Controversy around priors being subjective: [Prosecutor's Fallacy](https://towardsdatascience.com/the-prosecutors-fallacy-cb0da4e9c039)\n",
    "- Add a partitioning event and have the students calculate the posterior based on that - Shreyas\n",
    "- Bayes proof for the generalized case - NJ\n",
    "- Generalized case example - Together\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Question ðŸ¤”\n",
    "> Tell me something that I thought I could answer using what Ravi taught last lesson but in reality is hard to do without Bayes Theorem\n",
    "\n",
    "- That should explian why calculating probabilities in one direction is harder than calculating it the other way\n",
    "- **OR** position it as hypothesis and evidence where only one direction is what is interesting to look at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P(\\text{at least one shipment is air}) &= \\frac{3}{4}\\\\\n",
    "P(\\text{the second shipment is air}) &= \\frac{1}{2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Posterior Probability: \n",
    "- Conditional Probability: \n",
    "- Prior Probability: \n",
    "- Marginal Probability: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Law of Total Probability ðŸ“œ\n",
    "\n",
    "Now we need connect conditional and unconditional probabilities. We do this with **the Law of Total Probability** (LOTP). \n",
    "\n",
    "<br>\n",
    "\n",
    "Once we understand this, we will have all the required tools to prove the all-important identity: **Bayes' Theorem**, which we'll leave for next time. \n",
    "\n",
    "<br>\n",
    "\n",
    "You'll also have the tools to deal with conditioning on multiple events/pieces of information since the concepts translate generally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Law of Total Probability** is an incredibly useful problem solving tool. Formally stated, it says:\n",
    "\n",
    "$$\n",
    "\\text{If }A_i,...,A_n \\text{ is a partition of the sample space }S \\text{, then }P(B) = \\sum_{i=1}^{n}{P(B|A_i)P(A_i)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is likely better illustrated with a picture:\n",
    "\n",
    "![Partition of B by A](./LOTP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, your turn to practice!\n",
    "\n",
    "**Question**: \n",
    "\n",
    "> What's $P(\\text{TPEB})$. \n",
    "\n",
    "Partition the data and use LOTP so you can calculate it. Check against the data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-f57864d604cf>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-f57864d604cf>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    p_Air =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## TODO: Demonstrate LOTP on our data; start with tradelane_mode_xt\n",
    "\n",
    "# This is the denominator to convert cardinality of sets to probabilities\n",
    "# (per the Naive Definition of Probability)\n",
    "S = tradelane_mode_xt.sum().sum()\n",
    "\n",
    "# Show that p_TPEB_by_LTOP == p_TPEB\n",
    "p_TPEB = tradelane_mode_xt.loc['TPEB',:].sum()/S\n",
    "\n",
    "p_Air = \n",
    "\n",
    "p_not_Air = 1 - p_Air\n",
    "\n",
    "p_TPEB_given_Air = \n",
    "\n",
    "p_TPEB_given_not_Air = \n",
    "\n",
    "\n",
    "p_TPEB_by_LOTP = \n",
    "\n",
    "# Check if our answer is right   \n",
    "print(f\"Our Answer: {p_TPEB_by_LOTP:.5%}\")\n",
    "print(f\"Expected Answer: {p_TPEB:.5%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability!  \n",
    "\n",
    "Let's build up the intuition behind Bayes Theorem.\n",
    "\n",
    "From last time we know:\n",
    "\n",
    "The probability of two events A and B happening, $P(A \\cap B)$ , is the probability of $A$, $P(A)$, times the probability of B given that A has occurred, $P(B \\mid A)$. \n",
    "\n",
    "$P(A \\cap B)$ = $P(A)P(B \\mid A)$\n",
    "\n",
    "On the other hand, the probability of A and B is also equal to the probability\n",
    "of B times the probability of A given B.\n",
    "\n",
    "$P(A \\cap B)$ = $P(B)P(A \\mid B)$\n",
    "\n",
    "Equating the two yields:\n",
    "\n",
    "$P(B)P(A \\mid B)$ = $P(A)P(B \\mid A)$\n",
    "\n",
    "and thus\n",
    "\n",
    "$P(A \\mid B) = \\frac{P(A) P(B \\mid A)} {P(B)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method that we have just proved above is due to the Reverend [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes) (1701-1761). His method solved what was called an \"inverse probability\" problem: given new data, how can you update chances you had found earlier? Though Bayes lived three centuries ago, his method is widely used now in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A|B) = 0.339%\n"
     ]
    }
   ],
   "source": [
    "# calculate the probability using Bayes Theorem\n",
    "# Handy Function we can use!!\n",
    "\n",
    "# calculate P(A|B) given P(A), P(B|A), P(B|not A)\n",
    "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
    "    # calculate P(not A)\n",
    "    not_a = 1 - p_a\n",
    "    # calculate P(B)\n",
    "    p_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
    "    # calculate P(A|B)\n",
    "    p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "    return p_a_given_b\n",
    " \n",
    "# P(A)\n",
    "p_a = 0.0002\n",
    "# P(B|A)\n",
    "p_b_given_a = 0.85\n",
    "# P(B|not A)\n",
    "p_b_given_not_a = 0.05\n",
    "# calculate P(A|B)\n",
    "result = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)\n",
    "# summarize\n",
    "print('P(A|B) = %.3f%%' % (result * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Rule of the General Case ###\n",
    "In general, if the entire outcome space can be partitioned into events $A_1, A_2 \\ldots , A_n$, and $B$ is an event of positive probability, then for each $i$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(A_i \\mid B) &= \\frac{P(A_iB)}{P(B)} ~~~~ \\text{(division rule)} \\\\ \\\\\n",
    "&= \\frac{P(A_iB)}{\\sum_{j=1}^n P(A_j B)} ~~~~ \\text{(the }A_j\\text{'s partition the whole space)} \\\\ \\\\\n",
    "&= \\frac{P(A_i)P(B \\mid A_i)}{\\sum_{j=1}^n P(A_j)P(B \\mid A_j)} ~~~~\n",
    "\\text{(multiplication rule)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This calculation is an application of the division rule in a setting where the events $A_1, A_2, \\ldots , A_n$ can be thought of as the results of an \"earlier\" stage of an experiment and $B$ the result of a \"later\" stage. The calculation allows us to find \"backwards in time\" conditional chances of an earlier event given a later one, by writing the chance in terms of the \"forwards in time\" conditional chances of the later event given the earlier ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
